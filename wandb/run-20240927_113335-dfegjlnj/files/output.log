 18%|████████████████████████████▋                                                                                                                                    | 107/600 [16:00<1:13:44,  8.97s/it]
Epoch 1, Loss: 17091397.63157895, MSE Loss: 17063377.42105263, KLD Loss: 28020.403633418835
Epoch 11, Loss: 15443581.842105264, MSE Loss: 15443575.842105264, KLD Loss: 6.09828698007684
Epoch 21, Loss: 14349243.0, MSE Loss: 14349239.0, KLD Loss: 3.7019655328047905
Epoch 31, Loss: 13448320.421052631, MSE Loss: 13448318.421052631, KLD Loss: 2.063220275075812
Epoch 41, Loss: 12678620.210526315, MSE Loss: 12678619.210526315, KLD Loss: 1.1049523604543585
Epoch 51, Loss: 12030582.736842105, MSE Loss: 12030581.736842105, KLD Loss: 0.5881708044754831
Epoch 61, Loss: 11509428.157894736, MSE Loss: 11509428.157894736, KLD Loss: 0.3213340357730263
Epoch 71, Loss: 11052243.0, MSE Loss: 11052243.0, KLD Loss: 0.1849979601408306
Epoch 81, Loss: 10674231.52631579, MSE Loss: 10674231.52631579, KLD Loss: 0.11346877248663652
Epoch 91, Loss: 10363604.05263158, MSE Loss: 10363604.05263158, KLD Loss: 0.07284244738127056
Epoch 101, Loss: 10104678.210526315, MSE Loss: 10104678.210526315, KLD Loss: 0.04870133650930304
Traceback (most recent call last):
  File "/home/jechmate/diffAE-elibeams/vae.py", line 229, in <module>
  File "/home/jechmate/diffAE-elibeams/vae.py", line 189, in train
    # Accumulate losses
  File "/home/jechmate/anaconda3/envs/eli/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/jechmate/anaconda3/envs/eli/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/jechmate/anaconda3/envs/eli/lib/python3.10/site-packages/torch/optim/adam.py", line 234, in step
    adam(params_with_grad,
  File "/home/jechmate/anaconda3/envs/eli/lib/python3.10/site-packages/torch/optim/adam.py", line 300, in adam
    func(params,
  File "/home/jechmate/anaconda3/envs/eli/lib/python3.10/site-packages/torch/optim/adam.py", line 364, in _single_tensor_adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
KeyboardInterrupt
